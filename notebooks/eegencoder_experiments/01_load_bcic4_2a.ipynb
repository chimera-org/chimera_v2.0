{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8bLRxVT7CwhHNegyAt9yP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chimera-org/chimera_v2.0/blob/main/notebooks/eegencoder_experiments/01_load_bcic4_2a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone your GitHub repository\n",
        "# IMPORTANT: Replace 'YOUR_GITHUB_REPO_URL' with the actual URL of your repository.\n",
        "# This command will clone the repository into the /content/ directory.\n",
        "!git clone https://github.com/chimera-org/chimera_v2.0/\n",
        "\n",
        "print(\"‚úÖ Repository cloned.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZpZTaIqSCO_",
        "outputId": "d453fd24-d3d0-467e-9131-546b757ff9df"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'chimera_v2.0' already exists and is not an empty directory.\n",
            "‚úÖ Repository cloned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ad39aa7",
        "outputId": "e8373a77-f75e-425f-f3b2-7f97eed248d2"
      },
      "source": [
        "print(\"Contents of /content/chimera_v2.0/\")\n",
        "!ls -F /content/chimera_v2.0/\n",
        "\n",
        "print(\"\\nContents of /content/chimera_v2.0/src/\")\n",
        "!ls -F /content/chimera_v2.0/src/"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of /content/chimera_v2.0/\n",
            "architecture/\t    data/\t      Makefile\t      regulatory/\tsrc/\n",
            "benchmarks/\t    deployment/       models/\t      requirements.txt\ttests/\n",
            "clinical/\t    docs/\t      notebooks/      research/\n",
            "CODE_OF_CONDUCT.md  GOVERNANCE.md     pyproject.toml  scripts/\n",
            "config/\t\t    ISSUE_TRACKER.md  quality/\t      security/\n",
            "CONTRIBUTING.md     LICENSE\t      README.md       SECURITY.md\n",
            "\n",
            "Contents of /content/chimera_v2.0/src/\n",
            "chimera/  eegencoder/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mgrsLYd4MvZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9366e6f5-0f3a-44af-a486-b1f773892662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Environment ready\n"
          ]
        }
      ],
      "source": [
        "# Setup and Imports\n",
        "\"\"\"\n",
        "BCI Competition IV 2a - Data Loading Verification\n",
        "Purpose: Verify dataset integrity before training EEGEncoder\n",
        "\"\"\"\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install minimal dependencies (Colab already has most)\n",
        "!pip install mne numpy scipy matplotlib tqdm --quiet\n",
        "\n",
        "# Add src to path\n",
        "import sys\n",
        "sys.path.append('/content/chimera_v2.0/src/') # Updated path to cloned repo\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from eegencoder.data_loader import BCIC4_2A_Loader, verify_dataset\n",
        "\n",
        "print(\"‚úÖ Environment ready\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39f07a15",
        "outputId": "d548abe9-ffbc-4f6a-f7ad-2bfe14417dff"
      },
      "source": [
        "# Creating target directory where GDF files are expected to be\n",
        "import os\n",
        "\n",
        "extraction_path = \"/content/drive/MyDrive/BCI_IV_2a/\"\n",
        "if not os.path.exists(extraction_path):\n",
        "    os.makedirs(extraction_path)\n",
        "    print(f\"Created directory: {extraction_path}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {extraction_path}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory already exists: /content/drive/MyDrive/BCI_IV_2a/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a1ac9ca",
        "outputId": "cfdb7c46-b552-4a8d-e05a-83efa9718585"
      },
      "source": [
        "# Extracting zip files into the target directory\n",
        "zip_file_path = \"/content/drive/MyDrive/Motor_Imagery_Datasets/OpenBCI/BCI_cIV_2a/BCICIV_2a_gdf.zip\"\n",
        "extraction_path = \"/content/drive/MyDrive/BCI_IV_2a/\"\n",
        "\n",
        "# Unzip the file\n",
        "!unzip -o \"{zip_file_path}\" -d \"{extraction_path}\"\n",
        "\n",
        "print(f\"‚úÖ Dataset extracted to {extraction_path}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Motor_Imagery_Datasets/OpenBCI/BCI_cIV_2a/BCICIV_2a_gdf.zip\n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A01E.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A01T.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A02E.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A02T.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A03E.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A03T.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A04E.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A04T.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A05E.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A05T.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A06E.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A06T.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A07E.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A07T.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A08E.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A08T.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A09E.gdf  \n",
            "  inflating: /content/drive/MyDrive/BCI_IV_2a/A09T.gdf  \n",
            "‚úÖ Dataset extracted to /content/drive/MyDrive/BCI_IV_2a/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Loader and Verify Subjects\n",
        "# Create loader instance\n",
        "loader = BCIC4_2A_Loader(data_path=\"/content/drive/MyDrive/BCI_IV_2a/\")\n",
        "\n",
        "# Verify first 3 subjects (training data)\n",
        "print(\"=== TRAINING DATA VERIFICATION ===\")\n",
        "for subj_id in [1, 2, 3]:\n",
        "    X, y = verify_dataset(loader, subject_id=subj_id)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "MVcDExqwXCbZ",
        "outputId": "add448cf-de1d-432d-c066-6908250a6b98"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TRAINING DATA VERIFICATION ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Event time samples were not unique. Consider setting the `event_repeated` parameter.\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4025559950.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== TRAINING DATA VERIFICATION ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubj_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverify_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubj_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/chimera_v2.0/src/eegencoder/data_loader.py\u001b[0m in \u001b[0;36mverify_dataset\u001b[0;34m(loader, subject_id)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mverify_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;34m\"\"\"Load one subject and print diagnostics\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_subject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úÖ Subject {subject_id:02d} loaded successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/chimera_v2.0/src/eegencoder/data_loader.py\u001b[0m in \u001b[0;36mload_subject\u001b[0;34m(self, subject_id, training)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Create epochs (trials)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# tmin=0 (cue onset), tmax=4 (4 seconds after cue)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         epochs = Epochs(raw, events, event_id=self.event_id,\n\u001b[0m\u001b[1;32m     68\u001b[0m                        \u001b[0mtmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                        preload=True, verbose=False)\n",
            "\u001b[0;32m<decorator-gen-372>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, raw, events, event_id, tmin, tmax, baseline, picks, preload, reject, flat, proj, decim, reject_tmin, reject_tmax, detrend, on_missing, reject_by_annotation, metadata, event_repeated, verbose)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mne/epochs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, raw, events, event_id, tmin, tmax, baseline, picks, preload, reject, flat, proj, decim, reject_tmin, reject_tmax, detrend, on_missing, reject_by_annotation, metadata, event_repeated, verbose)\u001b[0m\n\u001b[1;32m   3592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3593\u001b[0m         \u001b[0;31m# call BaseEpochs constructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3594\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m   3595\u001b[0m             \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3596\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-356>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, info, data, events, event_id, tmin, tmax, baseline, raw, picks, reject, flat, decim, reject_tmin, reject_tmax, detrend, proj, on_missing, preload_at_end, selection, drop_log, filename, metadata, event_repeated, raw_sfreq, annotations, verbose)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mne/epochs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_handle_event_repeated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mne/epochs.py\u001b[0m in \u001b[0;36m_handle_event_repeated\u001b[0;34m(events, event_id, event_repeated, selection, drop_log)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0mdrop_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevent_repeated\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;34m\"Event time samples were not unique. Consider \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;34m'setting the `event_repeated` parameter.\"'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Event time samples were not unique. Consider setting the `event_repeated` parameter.\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82125d2e",
        "outputId": "93e9de39-46e7-40e1-bf7f-63f98d048552"
      },
      "source": [
        "file_path = '/content/chimera_v2.0/src/eegencoder/data_loader.py'\n",
        "\n",
        "print(f\"--- Contents of {file_path} ---\")\n",
        "with open(file_path, 'r') as f:\n",
        "    content = f.read()\n",
        "    print(content)\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"\\nPlease look for the `Epochs(...)` constructor call and check if `event_repeated='merge'` is present in its arguments.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Contents of /content/chimera_v2.0/src/eegencoder/data_loader.py ---\n",
            "\"\"\"\n",
            "BCI Competition IV 2a Data Loader\n",
            "Optimized for Raspberry Pi deployment (minimal dependencies)\n",
            "\"\"\"\n",
            "\n",
            "import numpy as np\n",
            "import scipy.io as sio\n",
            "from mne.io import read_raw_gdf\n",
            "from mne import events_from_annotations, Epochs\n",
            "import warnings\n",
            "warnings.filterwarnings('ignore')\n",
            "\n",
            "class BCIC4_2A_Loader:\n",
            "    \"\"\"\n",
            "    Minimal loader for BCI Competition IV dataset 2a\n",
            "    Returns NumPy arrays directly (no MNE objects for Pi compatibility)\n",
            "    \"\"\"\n",
            "    \n",
            "    def __init__(self, data_path=\"/content/drive/MyDrive/BCI_IV_2a/\"):\n",
            "        \"\"\"\n",
            "        Args:\n",
            "            data_path: Path to folder containing A01T.gdf, A01E.gdf, etc.\n",
            "        \"\"\"\n",
            "        self.data_path = data_path\n",
            "        self.sfreq = 250  # Sampling frequency (Hz)\n",
            "        self.n_channels = 22\n",
            "        self.n_trials = 288\n",
            "        self.trial_length = 4  # seconds\n",
            "        \n",
            "        # Event codes: corrected for BCI IV 2a\n",
            "        self.event_id = {\n",
            "            'left_hand': 1,\n",
            "            'right_hand': 2,\n",
            "            'foot': 3,\n",
            "            'tongue': 4\n",
            "        }\n",
            "        \n",
            "    def load_subject(self, subject_id, training=True):\n",
            "        \"\"\"\n",
            "        Load a single subject's data\n",
            "        \n",
            "        Args:\n",
            "            subject_id: Integer 1-9\n",
            "            training: Bool (True for training session, False for evaluation)\n",
            "            \n",
            "        Returns:\n",
            "            X: np.array (n_trials, n_channels, n_samples) - EEG data\n",
            "            y: np.array (n_trials,) - Labels (0-3)\n",
            "        \"\"\"\n",
            "        suffix = 'T' if training else 'E'\n",
            "        filename = f\"{self.data_path}A{subject_id:02d}{suffix}.gdf\"\n",
            "        \n",
            "        # Load raw GDF file\n",
            "        raw = read_raw_gdf(filename, preload=True, verbose=False)\n",
            "        \n",
            "        # Apply bandpass filter (4-38 Hz) - critical for motor imagery\n",
            "        raw.filter(l_freq=4, h_freq=38, method='iir', verbose=False)\n",
            "        \n",
            "        # Extract events (trial markers)\n",
            "        events, event_dict = events_from_annotations(raw, verbose=False)\n",
            "        \n",
            "        # BCI IV 2a specific: event codes are offset by 2 in the file\n",
            "        events[:, 2] = events[:, 2] - 2\n",
            "        \n",
            "        # Create epochs (trials)\n",
            "        # tmin=0 (cue onset), tmax=4 (4 seconds after cue)\n",
            "        epochs = Epochs(raw, events, event_id=self.event_id,\n",
            "                       tmin=0, tmax=4, baseline=None,\n",
            "                       preload=True, verbose=False)\n",
            "        \n",
            "        # Convert to NumPy arrays\n",
            "        X = epochs.get_data()  # Shape: (n_trials, 22, 1000)\n",
            "        y = epochs.events[:, -1] - 1  # Convert to 0-indexed labels\n",
            "        \n",
            "        # Basic sanity checks\n",
            "        assert X.shape[1] == self.n_channels, f\"Expected 22 channels, got {X.shape[1]}\"\n",
            "        assert X.shape[2] == self.sfreq * self.trial_length, \"Trial length mismatch\"\n",
            "        \n",
            "        return X, y\n",
            "    \n",
            "    def load_all_subjects(self, subject_ids=None, training=True):\n",
            "        \"\"\"\n",
            "        Load multiple subjects for cross-subject training\n",
            "        \n",
            "        Args:\n",
            "            subject_ids: List of subject IDs (1-9). If None, loads all.\n",
            "            training: Bool (True for training, False for evaluation)\n",
            "            \n",
            "        Returns:\n",
            "            X: np.array (n_subjects * n_trials, n_channels, n_samples)\n",
            "            y: np.array (n_subjects * n_trials,)\n",
            "            groups: np.array (n_subjects * n_trials,) - Subject ID for each trial\n",
            "        \"\"\"\n",
            "        if subject_ids is None:\n",
            "            subject_ids = list(range(1, 10))\n",
            "            \n",
            "        X_list, y_list, groups_list = [], [], []\n",
            "        \n",
            "        for subj_id in subject_ids:\n",
            "            X_subj, y_subj = self.load_subject(subj_id, training=training)\n",
            "            X_list.append(X_subj)\n",
            "            y_list.append(y_subj)\n",
            "            groups_list.append([subj_id] * len(y_subj))\n",
            "            \n",
            "        X = np.concatenate(X_list, axis=0)\n",
            "        y = np.concatenate(y_list, axis=0)\n",
            "        groups = np.concatenate(groups_list, axis=0)\n",
            "        \n",
            "        return X, y, groups\n",
            "\n",
            "\n",
            "# Quick sanity check function\n",
            "def verify_dataset(loader, subject_id=1):\n",
            "    \"\"\"Load one subject and print diagnostics\"\"\"\n",
            "    X, y = loader.load_subject(subject_id)\n",
            "    \n",
            "    print(f\"‚úÖ Subject {subject_id:02d} loaded successfully\")\n",
            "    print(f\"üìä Data shape: {X.shape} (trials √ó channels √ó timepoints)\")\n",
            "    print(f\"üè∑Ô∏è  Label distribution: {np.bincount(y)}\")\n",
            "    print(f\"‚ö° Data range: [{X.min():.2f}, {X.max():.2f}] ¬µV\")\n",
            "    print(f\"üìà Mean amplitude: {X.mean():.2f} ¬± {X.std():.2f} ¬µV\")\n",
            "    \n",
            "    return X, y\n",
            "\n",
            "------------------------------------\n",
            "\n",
            "Please look for the `Epochs(...)` constructor call and check if `event_repeated='merge'` is present in its arguments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vRHvBPsjXF7I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}